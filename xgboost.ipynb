{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime as dt\n",
    "import scipy.signal as ss\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import group_lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ścieżka z modelami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"C:/Users/ndzad/Dropbox/MINI/WTUM/Modele/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie zbiorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ndzad\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "holidays_events = pd.read_csv(\"https://www.dropbox.com/s/bxyamlpevkiwwoq/holidays_events.csv?dl=1\")\n",
    "holidays_events[\"holiday_type\"] = holidays_events[\"type\"]\n",
    "holidays_events.drop([\"type\"],axis=1,inplace=True)\n",
    "oil = pd.read_csv(\"https://www.dropbox.com/s/l6ln0ztl4m0pw3a/oil.csv?dl=1\",parse_dates=['date'],index_col='date')\n",
    "oil2 = pd.read_csv(\"https://www.dropbox.com/s/l6ln0ztl4m0pw3a/oil.csv?dl=1\")\n",
    "sample_submission = pd.read_csv(\"https://www.dropbox.com/s/68jjl61x6u3klos/sample_submission.csv?dl=1\")\n",
    "stores = pd.read_csv(\"https://www.dropbox.com/s/lcxn6r9bs2exguq/stores.csv?dl=1\")\n",
    "test = pd.read_csv(\"https://www.dropbox.com/s/cvdo1gn7r5lu2uz/test.csv?dl=1\",index_col='id')\n",
    "train = pd.read_csv(\"https://www.dropbox.com/s/s8p2b5awnuqfk0d/train.csv?dl=1\",index_col='id')\n",
    "transactions = pd.read_csv(\"https://www.dropbox.com/s/92fij9bcwt0e0cj/transactions.csv?dl=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja przygotowująca dane objaśniające. Łączy zbiory i zmienia zmienne kategoryczne na zmienne numeryczna metodą One Hot Encoding, a także grupuje po dniu lub miesiącu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def przygotowanie_danych(df: pd.DataFrame, type, type_item):\n",
    "    \"\"\"\n",
    "    Funkcja łącząca zbiory i zmieniająca zmienne kategoryczne na zmienne numeryczne. Funkcja jest przewidziana\n",
    "    df - zbiór danych treningowych\n",
    "    type - dane sklepowe ('family') / dane rodzinne ('store_nbr')\n",
    "    type_item - konretny element zmiennej type np. dla type==\"family\" type_item==\"AUTOMOTIVE\"\n",
    "    \"\"\"\n",
    "    #Łaczymy z pozostałymi zbiorami\n",
    "\n",
    "    #Zbiór stores\n",
    "    df = df.merge(stores,how=\"left\",left_on=['store_nbr'],right_on=['store_nbr'])\n",
    "\n",
    "    #Zbiór transactions\n",
    "    df = df.merge(transactions,how=\"left\",left_on=['date','store_nbr'],right_on=['date','store_nbr'])\n",
    "\n",
    "    #Zbiór holidays_events\n",
    "    df = df.merge(holidays_events,how=\"left\",left_on=['date'],right_on=['date'])\n",
    "\n",
    "    #Dodanie oil\n",
    "    df = df.merge(oil2,how=\"left\",left_on=['date'],right_on=['date'])\n",
    "\n",
    "    #Interpolacja braków danych z oil\n",
    "    df.interpolate(method ='linear', limit_direction ='backward', inplace=True)\n",
    "\n",
    "    #Wybieramy family ze zbioru traningowego\n",
    "    df_fam = df.loc[(df[type]==type_item)]\n",
    "    \n",
    "    #Dodajemy zmienne na dzień tygodnia i na miesiąc\n",
    "    df_fam['dayofweek'] = pd.DatetimeIndex(df_fam['date']).dayofweek + 1\n",
    "    df_fam['month'] = pd.to_datetime(df_fam['date']).dt.month\n",
    "\n",
    "    #Usunięcie zmiennych, które nie będą zmieniane\n",
    "    df_fam.drop([type,\"description\",\"transferred\"],axis=1,inplace=True)\n",
    "    if type == \"family\":\n",
    "        type_opposite = \"store_nbr\"\n",
    "    elif type == \"store_nbr\":\n",
    "        type_opposite = \"family\"\n",
    "\n",
    "    #One Hot Encoding\n",
    "    df_fam = pd.get_dummies(df_fam,columns=[\"locale\",type_opposite,\"city\", \"state\", \"type\", \"cluster\",\"locale_name\", \"holiday_type\"],prefix=[\"locale\",type_opposite,\"city\", \"state\", \"type\", \"cluster\",\"locale_name\", \"holiday_type\"])\n",
    "    \n",
    "    #W zbiorze test jest tylko jeden miesiąc dlatego musimy zrobić technicnzy zabieg polegający na dodaniu kolumn z samymi zerami.\n",
    "    if 'sales' not in df_fam.columns:\n",
    "        missing_cols = ['sales','locale_National','locale_Regional', 'locale_name_Cayambe', 'locale_name_Cotopaxi', 'locale_name_Cuenca', 'locale_name_Ecuador', 'locale_name_El Carmen', 'locale_name_Esmeraldas', 'locale_name_Guaranda', 'locale_name_Guayaquil', 'locale_name_Ibarra', 'locale_name_Imbabura', 'locale_name_Latacunga', 'locale_name_Libertad', 'locale_name_Loja', 'locale_name_Machala', 'locale_name_Manta', 'locale_name_Puyo', 'locale_name_Quevedo', 'locale_name_Quito', 'locale_name_Riobamba', 'locale_name_Salinas', 'locale_name_Santa Elena', 'locale_name_Santo Domingo', 'locale_name_Santo Domingo de los Tsachilas', 'holiday_type_Additional', 'holiday_type_Bridge', 'holiday_type_Event', 'holiday_type_Transfer', 'holiday_type_Work Day']\n",
    "        for i in missing_cols:\n",
    "            df_fam[i] = 0\n",
    "\n",
    "    return(df_fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dane_drzewo(df: pd.DataFrame, type, type_item):\n",
    "    df2 =df.copy()\n",
    "    df2[\"family\"], uniques=pd.factorize(df2[\"family\"])\n",
    "    df2[\"city\"], uniques=pd.factorize(df2[\"city\"])\n",
    "    df2[\"state\"], uniques=pd.factorize(df2[\"state\"])\n",
    "    df2[\"type\"], uniques=pd.factorize(df2[\"type\"])\n",
    "    df2[\"locale\"], uniques=pd.factorize(df2[\"locale\"])\n",
    "    df2[\"locale_name\"], uniques=pd.factorize(df2[\"locale_name\"])\n",
    "    df2[\"holiday_type\"], uniques=pd.factorize(df2[\"holiday_type\"])\n",
    "    df2.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podział danych na część treningową i walidacyjną."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def podzial(df: pd.DataFrame, date='2016-06-01'):\n",
    "    \"\"\"\n",
    "    Funkcja dzieli zbiór df na cześć treningową i walidacyjną względem date.\n",
    "    \"\"\"\n",
    "    train = df.loc[(df['date']<date)].drop([\"date\"],axis=1)\n",
    "    test = df.loc[(df['date']>=date)].reset_index(drop=True).drop([\"date\"],axis=1)\n",
    "    return(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje modelujące"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_xgb(df1: pd.DataFrame,df2: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Funkcja tworzy model XGBoost na danych df1 i waliduje na danych df2.\n",
    "    \"\"\"\n",
    "    #Zbiory danych treningowy\n",
    "    X = df1.drop(['sales'],axis=1).copy()\n",
    "    Y = df1['sales'].copy()\n",
    "\n",
    "    #Zbiory danych walidacyjnych \n",
    "    X_val = df2.drop(['sales'],axis=1)\n",
    "    Y_val = df2['sales'].copy()\n",
    "\n",
    "    #Dopasowanie modelu\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X,Y)\n",
    "    Y_pred = model.predict(X_val)\n",
    "\n",
    "    #RMSE - jakość dopasowania\n",
    "    RMSE = mean_squared_error(Y_val,Y_pred,squared=False)/(np.mean(Y_val))\n",
    "    print(\"Model: \", model_name, \" RMSE: \",RMSE)\n",
    "\n",
    "    sciezka = ROOT + model_name + \".txt\"\n",
    "    model.save_model(sciezka)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model regresji liniowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ols(df1: pd.DataFrame,df2: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Funkcja tworzy model regresji liniowej metodą najmniejszych kwadratów na danych df1 i waliduje na danych df2.\n",
    "    \"\"\"\n",
    "    #Zbiory danych treningowy\n",
    "    X = df1.drop(['sales'],axis=1).copy()\n",
    "    Y = df1['sales'].copy()\n",
    "\n",
    "    #Zbiory danych walidacyjnych \n",
    "    X_val = df2.drop(['sales'],axis=1)\n",
    "    Y_val = df2['sales'].copy()\n",
    "\n",
    "    #Dopasowanie modelu\n",
    "    model = sm.OLS(Y,X).fit()\n",
    "    Y_pred = model.predict(X_val)\n",
    "\n",
    "    #RMSE - jakość dopasowania\n",
    "    RMSE = mean_squared_error(Y_val,Y_pred,squared=False)/(np.mean(Y_val))\n",
    "    print(\"Model: \", model_name, \" RMSE: \",RMSE)\n",
    "\n",
    "    sciezka = ROOT + model_name + \".txt\"\n",
    "    model.save_model(sciezka)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model drzewa regresyjnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tree(df1: pd.DataFrame,df2: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Funkcja tworzy model drzewa regresyjnego na danych df1 i waliduje na danych df2.\n",
    "    \"\"\"\n",
    "    #Zbiory danych treningowy\n",
    "    X = df1.drop(['sales'],axis=1).copy()\n",
    "    Y = df1['sales'].copy()\n",
    "\n",
    "    #Zbiory danych walidacyjnych \n",
    "    X_val = df2.drop(['sales'],axis=1)\n",
    "    Y_val = df2['sales'].copy()\n",
    "\n",
    "    #Dopasowanie modelu\n",
    "    model = DecisionTreeRegressor().fit(X,Y)\n",
    "    Y_pred = model.predict(X_val)\n",
    "\n",
    "    #RMSE - jakość dopasowania\n",
    "    RMSE = mean_squared_error(Y_val,Y_pred,squared=False)/(np.mean(Y_val))\n",
    "    print(\"Model: \", model_name, \" RMSE: \",RMSE)\n",
    "\n",
    "    sciezka = ROOT + model_name + \".txt\"\n",
    "    model.save_model(sciezka)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_group_lasso(df1,df2, model_name: str):\n",
    "    \"\"\"\n",
    "    Funkcja tworzy model grupowego lasso na danych df1 i waliduje na danych df2.\n",
    "    \"\"\"\n",
    "    #Zbiory danych treningowy\n",
    "    X = df1.drop(['sales'],axis=1).copy()\n",
    "    Y = df1['sales'].copy()\n",
    "\n",
    "    #Zbiory danych walidacyjnych \n",
    "    X_val = df2.drop(['sales'],axis=1)\n",
    "    Y_val = df2['sales'].copy()\n",
    "\n",
    "    #Dopasowanie modelu\n",
    "    model = group_lasso.GroupLasso(groups=[-1,-1, 1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
    "        2,  2,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
    "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
    "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
    "        4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
    "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,\n",
    "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,\n",
    "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
    "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
    "        9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,-1]).fit(X,Y)\n",
    "    Y_pred = model.predict(X_val)\n",
    "\n",
    "    #RMSE - jakość dopasowania\n",
    "    RMSE = mean_squared_error(Y_val,Y_pred,squared=False)/(np.mean(Y_val))\n",
    "    print(\"Model: \", model_name, \" RMSE: \",RMSE)\n",
    "\n",
    "    sciezka = ROOT + model_name + \".txt\"\n",
    "    model.save_model(sciezka)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funckja do predykcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predykcja (df: pd.DataFrame, model: xgb.XGBRegressor, folder: str):\n",
    "    \"\"\"\n",
    "    Funkcja wykonuje predykcje modelu model na zbiorze df i odpisuje w formacie .csv do folderu folder.\n",
    "    \"\"\"\n",
    "    mod2 = xgb.XGBRegressor()\n",
    "    mod2.load_model(ROOT+model+\".txt\")\n",
    "    df_res =df.copy()\n",
    "    pred = mod2.predict(df)\n",
    "    df_res[\"sales_pred\"] = pred\n",
    "    pred_csv = df_res.to_csv(folder+model+\".csv\",index=False)\n",
    "\n",
    "    return(pred_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train2,test2) = podzial(przygotowanie_danych(train,'family','AUTOMOTIVE'),'2016-06-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2647430624584262\n"
     ]
    }
   ],
   "source": [
    "RMSE_xgb = model_xgb(train2,test2,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train3,test3) = podzial(przygotowanie_danych(train,'store_nbr',1),'2016-06-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6198961685176666\n"
     ]
    }
   ],
   "source": [
    "(model_xgb, RMSE_xgb) = model_xgb(train3,test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dla każdej rodziny. Modele odpisywane są do ROOT - ścieżki podanej na początku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = np.array(train[\"family\"].unique())\n",
    "family_RMSEs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  AUTOMOTIVE  RMSE:  0.6861653699517859\n",
      "Model:  BABY CARE  RMSE:  3.4948295658920756\n",
      "Model:  BEAUTY  RMSE:  0.7205881764239868\n",
      "Model:  BEVERAGES  RMSE:  0.31779410053589807\n",
      "Model:  BOOKS  RMSE:  3.994430513647246\n",
      "Model:  BREAD_BAKERY  RMSE:  0.2647430624584262\n",
      "Model:  CELEBRATION  RMSE:  1.170489191002666\n",
      "Model:  CLEANING  RMSE:  0.30010944377920146\n",
      "Model:  DAIRY  RMSE:  0.22848451644533507\n",
      "Model:  DELI  RMSE:  0.25711408962040094\n",
      "Model:  EGGS  RMSE:  0.5358559262506273\n",
      "Model:  FROZEN FOODS  RMSE:  1.183994333148742\n",
      "Model:  GROCERY I  RMSE:  0.34631785551534033\n",
      "Model:  GROCERY II  RMSE:  0.8921728224904292\n",
      "Model:  HARDWARE  RMSE:  1.2474795856009246\n",
      "Model:  HOME AND KITCHEN I  RMSE:  1.2312579643318984\n",
      "Model:  HOME AND KITCHEN II  RMSE:  1.395166723816384\n",
      "Model:  HOME APPLIANCES  RMSE:  2.027397594974563\n",
      "Model:  HOME CARE  RMSE:  0.3832039660347386\n",
      "Model:  LADIESWEAR  RMSE:  1.0003645281008315\n",
      "Model:  LAWN AND GARDEN  RMSE:  1.5703395588668807\n",
      "Model:  LINGERIE  RMSE:  1.6117339198708882\n",
      "Model:  LIQUOR,WINE,BEER  RMSE:  0.8290302418644222\n",
      "Model:  MAGAZINES  RMSE:  1.1906109680185522\n",
      "Model:  MEATS  RMSE:  1.628283326404564\n",
      "Model:  PERSONAL CARE  RMSE:  0.3576669348686532\n",
      "Model:  PET SUPPLIES  RMSE:  0.9998124164531473\n",
      "Model:  PLAYERS AND ELECTRONICS  RMSE:  0.9308743132629435\n",
      "Model:  POULTRY  RMSE:  0.45129454942475444\n",
      "Model:  PREPARED FOODS  RMSE:  0.39388841380161654\n",
      "Model:  PRODUCE  RMSE:  0.5170562314183849\n",
      "Model:  SCHOOL AND OFFICE SUPPLIES  RMSE:  4.2863020410426875\n",
      "Model:  SEAFOOD  RMSE:  0.46708949127934346\n"
     ]
    }
   ],
   "source": [
    "for i in families:\n",
    "    (train2,test2) = podzial(przygotowanie_danych(train,'family',i),'2016-06-01')\n",
    "    RMSE_xgb = model_xgb(train2,test2,i)\n",
    "    family_RMSEs.append(RMSE_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predykcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = przygotowanie_danych(test,\"family\",\"AUTOMOTIVE\")\n",
    "t.drop([\"date\",\"sales\"],axis=1,inplace=True)\n",
    "time = dt.datetime.now().strftime(\"%Y_%m_%d-%H-%M-%S\")\n",
    "folder2 = ROOT+\"predict_\"+time+\"/\"\n",
    "os.makedirs(folder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = xgb.XGBRegressor()\n",
    "for i in families:\n",
    "    mod2.load_model(ROOT+i+\".txt\")\n",
    "    mod2.predict(test2.drop([\"sales\"],axis=1))\n",
    "    predykcja(t,i,folder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dla każdego sklepu. Modele odpisywane są do ROOT - ścieżki podanej na początku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = np.array(train[\"store_nbr\"].unique())\n",
    "stores_RMSEs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stores:\n",
    "    (train2,test2) = podzial(przygotowanie_danych(train,'store_nbr',i),'2016-06-01')\n",
    "    RMSE_xgb = model_xgb(train2,test2,i)\n",
    "    stores_RMSEs.append(RMSE_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outliery (zrobione)\n",
    "- Grupowanie po family,typie sklep (uśrednienie sprzedaży po dacie)\n",
    "- Agregacja i predykcja na miesiąc "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbce44053a392e2d3acfeeaf9c14acc950329011ff8566e096397dd8c7fd702a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
